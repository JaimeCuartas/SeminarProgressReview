

\textbf{Transition to Explainability for Pushdown Automata decisions}
%
Following the confirmation of candidacy,
a natural progression from the study of
Finite Automata is to consider computational
models with greater expressive power.
the research scope has expanded to include
Pushdown Automata (PDA).
%

\textbf{The CYK-Based Explanation Engine}

To extract formal explanations from these Context-Free
structures, we have developed a novel adaptation of the
CYK (Cocke-Younger-Kasami) algorithm.

While standard CYK is primarily used for parsing,
we utilize the resulting triangular parsing table
as a search space for explanations.

By analyzing partial trees within the CYK table,
for a a rejected string $w$ and a PDA $\mathcal{A}$
we can identify:

\begin{itemize}
  \item \emph{Minimal Abductive Explanations:}
  minimal subsets of input tokens (leaves in the parse tree)
  such that is impossible 
  for the start symbol $S$ to be derivable
  (does not appear in the top cell)
  \item \emph{Minimal Contrastive Explanations:}
  minimal set of edits (substitutions)
  required to ``repair'' the CYK table such
  that the start symbol $S$ is derivable
  (appears in the top cell).

\end{itemize}


    Minimal Contrastive Explanations: In the case of rejection, the 
Definition environment.

The Grammar $\mathcal{A}$ PCFG is defined as a tuple $G = (V, \Sigma, R, S, P)$, where:

\begin{itemize}
    \item $V$ is a finite set of non-terminal symbols.
    \item $\Sigma$ is a finite set of terminal symbols (the alphabet).
    \item $R$ is a finite set of production rules.
    \item $S \in V$ is the start symbol.
    \item $P: R \to [0, 1]$ is a probability function such that for each
$A \in V$, $\sum_{A \to \alpha \in R} P(A \to \alpha) = 1$.
\end{itemize}

The Rejected WordLet $w = \sigma_1\sigma_2...\sigma_n$
be a string in $\Sigma^*$.We say $w$ is rejected if $w \notin L(G)$, where $L(G)$
is the language generated by the grammar.


\textbf{Definition:} Contrastive Set. For a rejected word $w$ of length $n$,
a set of indices $I \subseteq \{1, \dots, n\}$ is a Contrastive Explanation if:

\begin{itemize}
    \item Feasibility: There exists a word $w' \in L(G)$
    such that $w$ and $w'$ differ only at indices $i \in I$.
    Formally, $\forall j \notin I, \sigma_j = \sigma'_j$.
    \item Minimality: No proper subset $I' \subset I$ satisfies the feasibility condition.
\end{itemize}

\textbf{Example:} For $w = ( )))$, the index set $I = \{2\}$ is a contrastive explanation
because changing index 2 to $)$ results in $w' = ( ( )) \in L(G)$, and the empty set
$\emptyset$ is not feasible.



\textbf{Introducing Probabilistic Preference}. In a PCFG, not all accepted words $w'$ are equal.
We can rank explanations by the likelihood of the correction they enable.

\textbf{The Scoring Function} 
% For any word $w' \in L(G)$, the probability $P(w')$ is the sum
% of the probabilities of all possible parse trees for
% $w'$.
For any word $w' \in L(G)$, the score $P(w')$ is the maximum probability
among all possible parse trees $T$ that yield $w'$ (Viterbi Algorithm):
$$P(w') = \max_{T \in \text{Trees}(w')} P(T)$$
\textbf{Optimal Contrastive Explanation} Given a rejected word $w$,
an explanation $I_1$ is probabilistically superior to $I_2$ if the best
correction enabled by $I_1$ is more likely than that of $I_2$.
We define the Explanation Weight as:
$$\text{Weight}(I) = \max \{ P(w') \mid w' \text{ matches } w \text{ except at indices } I, w' \in L(G) \}$$


\textbf{The Extended CYK Table}

Standard CYK populates a 3D table $T[i, j, A]$, representing the maximum probability
that non-terminal $A$ derives the substring from index $i$ to $j$.

For a word $w = \sigma_1\sigma_2...\sigma_n$ and an index set $I$:

\begin{itemize}
    \item \textbf{Base Case (Length 1)}
    For each position $i \in \{1, \dots, n\}$ and each non-terminal $A$:    
    \begin{itemize}
        \item If $i \notin I$ (Fixed):
        $$T[i, i, A] = P(A \to \sigma_i)$$
        (If no such rule exists, the probability is 0).
        \item If $i \in I$ (in cxp):
        $$T[i, i, A] = \sum_{\sigma \in \Sigma}  P(A \to \sigma) $$
        This selects the most likely terminal that $A$ can produce at that ``broken" index.
    \end{itemize}
    \item \textbf{Recursive Step (Length $l > 1$)}
    For each length $l$ from 2 to $n$, each starting position $i$ from 1 to $n-l+1$, and each non-terminal $A$:$$T[i, l, A] = \max_{A \to BC \in R} \left( \max_{1 \le k < l} \{ P(A \to BC) \cdot T[i, k, B] \cdot T[i+k, l-k, C] \} \right)$$
\end{itemize}

\textbf{Heatmap for Contrastive Explanations}


\textbf{Theorem}: Given a word $w=\sigma_1\sigma_1\dots \sigma_n$, and grammar $G$, such that $w \notin L(G)$.
Let $\mathcal{E}$ be the set of all contrastive explanations for $w$.
If $H$ is a hitting set of all contrastive explanations $\mathcal{E}$,
then no word $w'$ that keeps the indices in $H$ fixed to their original values in $w$
can be accepted by the grammar.
Mathematically:

If $\forall i \in H, \sigma'_i = \sigma_i$, then $w' \notin L(G)$.

Proof by Contradiction

\textbf{Step 1:} Assume the negation. Assume there exists a word 
$w' \in L(G)$ such that $w'$ agrees with the original rejected
word $w$ on all indices in the hitting set $H$.
$$\forall i \in H: \sigma'_i = \sigma_i$$

\textbf{Step 2:} Let $J$ be the set of indices where
$w'$ differs from $w$ ($J \cap H = \emptyset$).
$$J = \{ j \mid \sigma'_j \neq \sigma_j \}$$

\textbf{Step 3:} Relate to Contrastive Explanations.

Since $w' \in L(G)$ and it was formed by changing indices $J$ in $w$,
then by definition, $J$ is a "feasible" contrastive set.
It must either be a minimal contrastive
explanation or a superset of one.
$$\exists I \in \mathcal{E} \text{ such that } I \subseteq J$$

\textbf{Step 4:} The Contradiction.

We know from Step 2 that $J \cap H = \emptyset$.
Since $I \subseteq J$, it follows that $I \cap H = \emptyset$.
However, by definition, $H$ is a hitting set of $\mathcal{E}$,
meaning it must have a non-empty intersection with every
$I \in \mathcal{E}$.
$$H \cap I \neq \emptyset \quad (\text{Contradiction})$$

\textbf{Conclusion:} Our assumption that an accepted word $w'$ exists is false.
Therefore, the indices in $H$ effectively ``block" all possible paths to acceptance.
They are the necessary components of the error.

\begin{align*}
0.8 & \quad VP \to V \ NP   & 1   & \quad NP \to Det \ N & 0.1 & \quad N \to N \ PP   & 1 & \quad Det \to \text{the} & 0.6 & \quad N \to \text{man} \\
0.2 & \quad VP \to VP \ PP & 1   & \quad PP \to P \ NP  & 1   & \quad V \to \text{sees} & 1 & \quad P \to \text{with}   & 0.3 & \quad N \to \text{telescope}
\end{align*}

% \begin{forest}
% [VP
%   [VP
%     [V [sees]]
%     [NP
%       [Det [the]]
%       [N [man]]
%     ]
%   ]
%   [PP
%     [P [with]]
%     [NP
%       [Det [the]]
%       [N [telescope]]
%     ]
%   ]
% ]
% \end{forest}

% \begin{forest}
% [VP
%   [V [sees]]
%   [NP
%     [Det [the]]
%     [N
%       [N [man]]
%       [PP
%         [P [with]]
%         [NP
%           [Det [the]]
%           [N [telescope]]
%         ]
%       ]
%     ]
%   ]
% ]
% \end{forest}

\begin{forest}
  for tree={
    s sep=45pt,
    l sep=25pt,
  }
[VP, label=right:{\textbf{0.2}}
  [VP, label=left:{\textbf{0.8}}
    [V, label=left:{\textbf{1.0}} [sees]]
    [NP, label=right:{\textbf{1.0}}
      [Det, label=left:{\textbf{1.0}} [the]]
      [N, label=right:{\textbf{0.6}} [man]]
    ]
  ]
  [PP, label=right:{\textbf{1.0}}
    [P, label=left:{\textbf{1.0}} [with]]
    [NP, label=right:{\textbf{1.0}}
      [Det, label=left:{\textbf{1.0}} [the]]
      [N, label=right:{\textbf{0.3}} [telescope]]
    ]
  ]
]
\end{forest}

P(t1) = 0.6 * 0.8 * 0.2 * 0.3 = 0.0288

\begin{forest}
  for tree={
    s sep=45pt,
    l sep=25pt,
  }
[VP, label=left:{\textbf{0.8}}
  [V, label=left:{\textbf{1.0}} [sees]]
  [NP, label=right:{\textbf{1.0}}
    [Det, label=left:{\textbf{1.0}} [the]]
    [N, label=right:{\textbf{0.1}}
      [N, label=left:{\textbf{0.6}} [man]]
      [PP, label=right:{\textbf{1.0}}
        [P, label=left:{\textbf{1.0}} [with]]
        [NP, label=right:{\textbf{1.0}}
          [Det, label=left:{\textbf{1.0}} [the]]
          [N, label=right:{\textbf{0.3}} [telescope]]
        ]
      ]
    ]
  ]
]
\end{forest}

P(t2) = 0.6 * 0.8 * 0.1 * 0.3 = 0.0144

p(VP,sees the man with the telescope) = 0.0288 + 0.0144 = 0.0432

If ``telescope'' were ``man''

P(t3) = 0.6 * 0.8 * 0.2 * 0.6 = 0.0576

P(t4) = 0.6 * 0.8 * 0.1 * 0.6 = 0.0288

sees the man with the ? = P(t1) + P(t2) + P(t3) + P(t4) = 0.1296



Given the grammar $G$ and the rejected word 
$w=\sigma_1\sigma_2\dots \sigma_n\not\in L(G)$.
Let $\mathcal{E} = \{I_1, I_2, \dots, I_k\}$ the collection of all 
Minimal Contrastive Explanations 

Each $I \in \mathcal{E}$: ``The error happened exactly in the set of index $I$''.
and $P(I)$ Is the sum up of probabilities of all possible trees that 
derivates accepted words $w'=\sigma'_1\sigma'_2\dots \sigma'_n$
where $\forall_{i\not\in I} \sigma'_i=\sigma_i$

$P(A)$: Probability to get an accepted Tree with a minimal cxp.
Every tree is disjoint.
$P(A) = P(I_1) + P(I_2) + \dots + P(I_k)$

$$P(I \mid A) = \frac{P(I)}{\sum_{J \in \mathcal{E}} P(J)}$$

$$P(\text{Error in } i \mid A) = \sum_{I \in \mathcal{E}} P(\text{Error in } i \mid I, A) P(I \mid A)$$

$$P(\text{Error in } i \mid I, A) = \mathbb{F}(i \in I) = \begin{cases} 1 & \text{if } i \in I \\ 0 & \text{if } i \notin I \end{cases}$$

\section{Future Work and Timeline to Completion}


% \begin{figure}[tbp]
%     \begin{center}
    
%     \begin{ganttchart}[expand chart=\textwidth,
%     hgrid,
%     vgrid,
%     x unit=0.8cm,
%     y unit chart=0.7cm,
%     time slot format=simple,
%     title label font=\bfseries\small,
%     group label font=\bfseries\small,
%     bar label font=\small,
%     milestone label font=\small\itshape]{1}{12}
%     %labels
%     \gantttitle{2026 (Year 2)}{4} 
%     \gantttitle{2027 (Year 3)}{4} 
%     \gantttitle{2028 (Final)}{4} \\
%     \gantttitlelist{1,...,12}{1} \\
%     % Phase 1: Theoretical Work
%     \ganttgroup{Theoretical Refinement}{1}{3} \\
%     \ganttbar{Formal Logic for Dynamic Env}{1}{2} \\
%     \ganttmilestone{Framework Finalization}{3} \\
%     % Phase 2: System Development
%     \ganttgroup{System Implementation}{4}{7} \\
%     \ganttbar{Explainer Algorithm Dev}{4}{6} \\
%     \ganttbar{Simulator Integration}{6}{7} \\
%     % Phase 3: Validation
%     \ganttgroup{Validation \& Testing}{8}{10} \\
%     \ganttbar{Case Study: Autonomous Agents}{8}{9} \\
%     \ganttbar{Comparative Analysis}{9}{10} \\
%     % Phase 4: Thesis
%     \ganttgroup{Thesis Completion}{10}{12} \\
%     \ganttbar{Final Drafting}{10}{11} \\
%     \ganttmilestone{Thesis Submission}{12}
    

%     \end{ganttchart}
%     \end{center}
%     \caption{Gantt Chart}

% \end{figure}


\chapter{Preliminary Results}

\section{Responsibility Attribution for Token Substitutions}

The linear system treats the probability of a CXP as a
shared resource among its constituent indices.
For each index $i$, the "Responsibility Score" $S_i$ is:
%
$$S_i = \sum_{j: i \in cxp_j} \frac{P(cxp_j)}{|cxp_j|}$$
%
Where $|cxp_j|$ is the size of the explanation (minimal set). 
This takes into account the fact that if an explanation requires
changing 5 tokens, the credit for that fix is
divided across those 5 tokens.

\section{Badness Attribution}

How bad is each token in causing the rejection of a word?

One string is accepted when every token is correct simultaneusly.

\begin{itemize}
    \item Let us define $v_i$ as the "badness" of token $i$,
    \item Then, $(1 - v_i)$ as the "goodness", the probability
    that token $i$ is correct.
    \item The probability that the entire word is correct (accepted)
    is the product of the probabilities that each token is correct:
    $P(\text{Acceptance}) = \prod (1 - v_i)$,
    \item Given a rejected word and a proposed CXP (repair) that
    modifies certain tokens, the probability that this CXP
    leads to acceptance is:
    . 
    And we know that the tokens that are not part of the cxp
    can be valid.
    $\forall_{i \notin CXP} (1 - v_i) = 1$ the goodness is 1.
    $P(\text{Acceptance with CXP}) = \prod (1 - v_i) = \prod_{i \in CXP} (1 - v_i)$,

\end{itemize}

\section{Example Probabilistic context-free grammar}

Let us consider a simple PCFG that generates balanced parentheses:

S - S S

S - L R

S - L SR

N - 'number'

L - '('

R - ')'

SR - S R

SR - N R


Each production rule has an associated probability:

P(S - S S) = 0.1

P(S - L R) = 0.2

P(S - L SR) = 0.7

P(N - 'number') = 1.0

P(L - '(') = 1.0

P(R - ')') = 1.0

P(SR - S R) = 0.1

P(SR - N R) = 0.9

consider the rejected string: "( number ) ) ( )"

There are two minimal contrastive explanations (CXPs) that can
repair this string:
CXP1: Change index 1 to \text{( ( ) ) ( )}

\begin{itemize}
    \item S - S S (0.1)
    \begin{itemize}
        \item S - L SR (0.7)
        \begin{itemize}
            \item L - ( (1.0)
            \item SR - S R (0.1)
            \begin{itemize}
                \item S - L R (0.2)
                \item R - ) (1.0)
            \end{itemize}
        \end{itemize}
        \item S - L R (0.2)
    \end{itemize}
\end{itemize}

0.1*0.7*0.1*0.2*0.2 = 0.000028

CXP2: Change indexes 3 and 4 to ( number ) ( number )


\section{Kaczmarz method}

% Each index $i$ has an associated "badness" $v_i$,
% representing the probability that the token at index $i$
% is incorrect or causes rejection.

Para justificar el uso de la multiplicación en tu modelo de
Rejection Attribution Score (RAS) y su relación con la "maldad"
($v_i$) en el contexto de tu doctorado, debes fundamentarlo en
tres pilares: la lógica booleana de los autómatas, la teoría de
probabilidad de las gramáticas (PCFG) y la tratabilidad matemática.
Aquí tienes la justificación técnica y académica:1. 
La lógica del "Y" (Intersección de Eventos)En la teoría de
lenguajes formales, para que una palabra sea aceptada por un
autómata (o generada por una gramática), todas las restricciones
estructurales deben cumplirse simultáneamente.
Es una condición de tipo AND

Si definimos $(1 - v_i)$ como la
probabilidad de que el índice $i$ sea "correcto" o "válido" según
las reglas de la gramática, la aceptación de la palabra completa
es el evento donde el índice 1 es válido Y el índice 2 es válido
Y así sucesivamente.

En teoría de probabilidad, la probabilidad de
la intersección de eventos independientes es el producto de sus
probabilidades individuales: 
$P(\text{Aceptación}) = P(\text{validez}_1) \cdot P(\text{validez}_2) \dots$

Por lo tanto, $P = \prod (1 - v_i)$
es la representación natural de una estructura donde el fallo de un
solo componente puede comprometer la aceptación total.



2. Justificación mediante el modelo de "Canal con Ruido"Para tu tesis,
puedes presentar la palabra rechazada como una versión "ruidosa" o "corrupta"
de una palabra válida.Justificación: Cada $v_i$ representa la probabilidad
de que haya ocurrido un error de transformación en el índice $i$.
Bajo esta premisa, la probabilidad de que una palabra reparada
(una explicación contrastiva como $[1, 3]$) sea la "verdadera" intención
del usuario depende de que los demás índices se mantengan correctos.
La multiplicación cuantifica la verosimilitud de esa reparación estructural.
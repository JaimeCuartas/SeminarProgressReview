\chapter{Progress}

\section{Model Proposal: Context-Free Grammar (CFG) Explanations}

The research has evolved
from the study of Finite Automata (FA)
to more expressive computational models.
%
While FA provided a baseline for explaining
sequential behaviors,
%
many complex problems require the model to
``remember'' an arbitrary number of previous
inputs in the sequence
to determine the validity of subsequent inputs

Consider the abstract language $L=\{a^{n}b^{n} \mid n \geq 1\}$,
which represents a sequence where every `a' must
be matched by a corresponding `b'.

\begin{itemize}
  %
  \item The Limitation: A standard
  Finite Automaton (FA) possesses no
  auxiliary memory.
  %
  Therefore, it cannot count the number
  of $a$'s to ensure they match the number
  of $b$'s once $n$ exceeds the number
  of states in the machine.
  \item The Explanation Failure:
  If an FA were used to validate such a sequence,
  it would process inputs locally.
  %
  Upon encountering a mismatch (e.g., $a^{4}b^{2}$),
  it might reject the sequence, but it lacks the
  structural context to generate a contrastive
  explanation such as:
  "The sequence is invalid because the third or fourth `a'
  was not closed by a matching `b'."
  Instead, it can only report a
  "transition failure" at the specific index,
  obscuring the root cause of the error.

\end{itemize}

\begin{itemize}
  \item \textbf{From FA to PDA:}
  We propose the use of Pushdown Automata (PDAs)
  to model decision-making processes with memory.

  Unlike FA, the addition of a stack
  allows the description of more complex
  languages.
  Here the challenge is how to explain
  PDA decisions.
\end{itemize}

\section{Motivation: The Gap between Detection and Explanation}

One well established applications of Context-Free Grammars
is in the design of programming languages and compilers.
Compilers are highly efficient at detecting when a
sequence of tokens fails to belong to a grammar.
%
However, a fundamental question is:
are they able of generating a useful \emph{explanation}
for why the failure occurred or how to fix it?

Consider the following C code, where a typo has
introduced a double opening bracket \texttt{\{\{} in the \texttt{for} loop:
%
\begin{lstlisting}[language=C]
int main(){
    for(int i=0; i<10; i++){{  // <--- Error: Double bracket
        printf("hello");
    }
}
\end{lstlisting}
%
When compiled (e.g., using GCC or Clang), the parser
consumes the input until it reaches
\textless\textless EOF\textgreater\textgreater (end-of-file),
finding only then that
EOF was not expected,
instead there is an incomplete structure, the token `\}'
is expected before \textless\textless EOF\textgreater\textgreater.
The resulting error message is:
%
\begin{verbatim}
error: expected '}' at end of input
   5 | }
     |  ^
\end{verbatim}
%
\noindent \textbf{The Explanation:}
While the compiler's output is correct,
the file ended while the stack still
contained an open brace that was not closed yet.
It is \textit{misleading}.
\begin{itemize}
  %
  \item \textbf{Root Cause:}
  The compiler points to line 5
  (the end of the file) as the location of the error.
  However, the root cause is located at line 2.
  %
  \item \textbf{Lack of Contrastive Reasoning:}
  A human (or a formal explainer) would identify
  that the input is ``almost correct''.
  %
  The explanation should not just report a
  missing symbol at the end,
  but rather propose a \textit{minimal correction}.
  %
\end{itemize}
%
In this case, the minimal correction is not to add a brace at the end,
but to remove the redundant opening brace at the loop initialization:
%
\begin{lstlisting}[language=C]
int main(){
    for(int i=0; i<10; i++){
        printf("hello");
    }
}
\end{lstlisting}
%
This discrepancy motivates the need for our proposed formal framework.
We aim to move beyond isolated decisions to
eplained decisions revealing
these minimal set of edits (Contrastive Explanations).

\section{Preliminaries} 

\subsection{Pushdown Automata and Context-Free Grammars}

To provide a foundation for the proposed explanation
extraction methods, 
standard definitions and notations
for Pushdown Automata and Context-Free Grammar are adopted
\cite{IntroAutomataTheory,HandbookTheoreticalContextFreeGrammar}.

\begin{definition}[Pushdown Automaton]
  A Pushdown Automaton (PDA) extends the capabilities of a
  Finite Automaton by incorporating an infinite memory stack.
  A PDA is formally defined as a 7-tuple
  $\mathcal{A}=(Q,\Sigma,V,\delta,q^0,v^0,F)$, where:
  \begin{itemize}
    %
    \item Q is a finite set of states.
    %
    \item $\Sigma$ is the input alphabet, a finite
    terminal alphabet.
    %
    \item $V$ is the stack alphabet, a finite
    nonterminal alphabet that can be pushed
    onto or popped from the stack.
    %
    \item $\delta:Q\times(\Sigma\cup\{\epsilon\}) \times V \rightarrow \mathcal{P}(Q \times V^*)$
    %
    \footnote{$\mathcal{P}$ denotes the power set
    (the set of all its subsets).
    %
    $V^*$ and $\Sigma^*$ denote the Kleene closures
    of the stack and input alphabets,
    respectively,
    %
    representing the sets of all finite strings formed
    by those alphabets.}
    %
    is the transition fuction.
    %
    It dictates how the machine transitions between states and
    modifies the stack based on the current state, input symbol,
    and the top symbol of the stack.
    %
    \item $q^0 \in Q$ is the initial state.
    \item $v^0 \in V$ is the initial pushdownstore symbol.
    \item $F\subseteq Q$ is the set of accepting states.
  \end{itemize}
%
  A configuration of $\mathcal{A}$ is a triple $c=(q, \gamma, x)$ in
  $Q\times V^*\times\Sigma^*$. And the automaton moves from c into configuration
  $c'=(q',\gamma', x')$, denoted as $c\vdash c'$ if:
  %
  \begin{itemize}
    %
    \item $\gamma = v \gamma_1(v\in V)$
    %
    \footnote{$\gamma_{1} \in V^*$ represents
    the remaining symbols on the
    stack below the top symbol $v$.
    %
    Thus, $\gamma$ represents the full current stack, formed by
    the top symbol $v$ (to be processed) and the rest of the stack $\gamma_1$.}
    %
    , $x=ax' (a\in \Sigma)$,
    $\gamma'= m \gamma_1(m\in V^*)$ and $(q', m)\in \delta(q, a, v)$, namely ``a-move'';
    %
    \item or $\gamma = v \gamma_1(v\in V)$, $x=x'$, $\gamma'= m \gamma_1(m\in V^*)$
    and $(q',m)\in \delta(q,\epsilon,v)$, namely ``$\epsilon$-move''.
  \end{itemize}
  %
  A \emph{word} is accepted by a pushdown automaton if, starting with an
  empty stack, there is a path through the automaton such
  that the automaton stops in an
  accepting state after the entire string has been read.
  %
  The \emph{language} recognized by a PDA $\mathcal{A}$
  is the set of all accepted words, denoted as $L(\mathcal{A})$.
  %
\end{definition}

The following PDA recognizes the
language of balanced parentheses
(a subset of the Dyck language)
%
\footnote{
  The Dyck language describes a set of strings
  with balanced and properly
  nested brackets (e.g., (), [], \{\})
  %
  \cite{HandbookTheoreticalContextFreeGrammar}.
  The example focuses solely on non-empty
  sequences of balanced ( and ).
}
%
and is used throughout the document
to illustrate the proposed ideas.
%

\begin{figure}[ht]
  \centering
    \input{Figures/PDA/parenthesis_PDA.tex}
    \caption{A Pushdown Automaton $\mathcal{A}$
    accepting the language of balanced parentheses.
    The symbol \$ is used as the bottom-of-stack marker.}
  \label{fig:pda_example}
\end{figure}
\begin{example}

  Let $\mathcal{A}$ be the PDA
  that accepts the language generated by $G$ in 
  \autoref{ex:grammar_balanced}.
  The PDA uses a stack to track the depth of nesting,
  pushing a symbol for every open parenthesis and popping
  for every closed one. $B$ represents \textit{Balanced} in $G$.
  
  \begin{itemize}
      \item States: $Q = \{q_0, q_1, q_f\}$, $q^0=q_0$, $F=\{q_f\}$
      \item Input Alphabet: $\Sigma = \{(,)\}$
      \item Stack Alphabet: $V = \{ (, ), B, \$ \}$, $v^0=B$
      \item Transitions:
      \begin{enumerate}
          \item $\delta(q_0, \epsilon, \epsilon) = \{(q_1, B\,\$)\}$ \quad (Initialize stack with Start Symbol)
          \item $\delta(q_1, \epsilon, B) = \{(q_1, (B)B),(q_1, (B)),(q_1, ()B),(q_1, ())\}$ \quad (Expand $B$)
          \item $\delta(q_1, (, () = \{(q_1, \epsilon)\}$ \quad (Match input `(' with stack `(')
          \item $\delta(q_1, ), )) = \{(q_1, \epsilon)\}$ \quad (Match input `)' with stack `)')
          \item $\delta(q_1, \epsilon, \$) = \{(q_f, \epsilon)\}$ \quad (Accept if bottom marker is reached)
      \end{enumerate}
  \end{itemize}

  \autoref{fig:pda_example} illustrates this PDA.
    The configuration history of $\mathcal{A}$ for the input \texttt{()()} is:
%
  \begin{equation}
    \begin{aligned}
        & (q_0, \texttt{()()}, \epsilon) \\
        \vdash \ & (q_1, \texttt{()()}, B\$) && \text{(Initialize)} \\
        \vdash \ & (q_1, \texttt{()()}, \texttt{()}B\$) && \text{(Expand } B \to \texttt{()}B \text{)} \\
        \vdash \ & (q_1, \texttt{)()}, \texttt{)}B\$) && \text{(Match `\texttt{(}')} \\
        \vdash \ & (q_1, \texttt{()}, B\$) && \text{(Match `\texttt{)}')} \\
        \vdash \ & (q_1, \texttt{()}, \texttt{()}\$) && \text{(Expand } B \to \texttt{()} \text{)} \\
        \vdash \ & (q_1, \texttt{)}, \texttt{)}\$) && \text{(Match `\texttt{(}')} \\
        \vdash \ & (q_1, \epsilon, \$) && \text{(Match `\texttt{)}')} \\
        \vdash \ & (q_{f}, \epsilon, \epsilon) && \text{(Accept)}
    \end{aligned}
  \end{equation}
\end{example}

\begin{definition}[Context-Free Grammar]
  A Context-Free Grammar (CFG)
  is defined as a 4-tuple $G=(V,\Sigma ,R,S)$, where:
  \begin{itemize}
    \item V (Variables/Non-terminals) is a finite set of variables (non-terminal symbols).
    \item $\Sigma$ (Terminals) is a finite set of terminal symbols, disjoint from V.
    \item R is a finite set of production rules of the
    form $A \rightarrow \alpha$, where $A\in V$ describes
    a variable and $\alpha\in(V \cup\Sigma)^*$ is a string
    of variables and terminals.
    \item $S\in V$ is the start variable.
  \end{itemize}
\end{definition}
%

A fundamental equivalence between CFGs and PDAs:
a language $L$ is context-free iff there exists a
PDA $\mathcal{A}$ such that $L(\mathcal{A})=L$
\cite{HandbookTheoreticalContextFreeGrammar, IntroAutomataTheory}.
%
This equivalence allows us to use grammar-based
parsing algorithms, such as CYK, to analyze the
behavior of PDAs.

\begin{example}
  \label{ex:grammar_balanced}
  Let $G=(\{Balanced\}, \{(,)\}, R, Balanced)$ be the grammar
  defined by the following 
  production rules $R$:
  %
  \begin{equation}
  \begin{aligned}
      Balanced &\to \texttt{(} \, Balanced \, \texttt{)} \, Balanced & \text{(Rule 1)}\\
      Balanced &\to \texttt{(} \, Balanced \, \texttt{)} & \text{(Rule 2)}\\
      Balanced &\to \texttt{(} \, \texttt{)} \, Balanced & \text{(Rule 3)}\\
      Balanced &\to \texttt{(} \, \texttt{)} & \text{(Rule 4)}
  \end{aligned}
  \end{equation}
  %
  This grammar generates the language of
  properly nested parentheses.
  For instance, the string \texttt{()()}
  can be derivated as follows.
  %
  \begin{equation}
  \begin{aligned}
      Balanced &\Rightarrow \texttt{( )} \, \mathbf{Balanced}  && \text{(Rule 3: parentheses and Balanced)} \\
               &\Rightarrow \texttt{( ) ( )} && \text{(Rule 4: Reduce `Balanced' to `()')}
  \end{aligned}
  \end{equation}
\end{example}

To analyze the behavior of the PDA
using grammar-based approaches, we must first standardize
the grammar structure (e.g. Chomsky Normal Form).
%
This enables the use of efficient
parsing algorithms 
like CYK (Cocke-Younger-Kasami) \cite{CYK1967, ParsingGuide}.

\subsection{Chomsky Normal Form}

Parsing algorithms often require the grammar to be in a
canonical form to ensure predictable execution complexity.

\begin{definition}[Chomsky Normal Form]
  A Context-Free Grammar $G=(V, \Sigma, R, S)$ is in
  \textit{Chomsky Normal Form} (CNF) \cite{IntroAutomataTheory} if every production rule
  in $R$ is of one of the following two forms:
  \begin{itemize}
      \item $A \rightarrow BC$ (where $A, B, C \in V$)
      \item $A \rightarrow a$ (where $a \in \Sigma$ and $a \neq \epsilon$)
  \end{itemize}
\end{definition}

For every CFG $G$ whose languge contains at least one string
other than $\epsilon$, then there is a grammar $G_1$
in Chomsky Normal Form.

\begin{example}
  \label{ex:cnf_conversion}
  Consider the grammar $G$ from \autoref{ex:grammar_balanced}.
  We transform $G$ into an
  equivalent grammar $G'=(V', \Sigma, R', S)$ in CNF.
  
  \noindent \textbf{Step 1: Terminals to Non-terminals.}
  To introduce variables $L$ and $R$ for terminals `(' and `)'.
  \[ L \to ( \quad \text{and} \quad R \to ) \]
  
  \noindent \textbf{Step 2: Binary decomposition.}
  To rewrite the original rules using new variables
  and break down productions into binary steps.
  
  The resulting production rules $R'$ are:
  \begin{center}
  \begin{tabular}{r r l l}
    Balanced  &$\to$&$ Nested\,Balanced$ & (Represents `( Balanced ) Balanced' ) \\
              &$\mid$& $Unclosed\,R$ & (Represents `( Balanced )' ) \\
              &$\mid$& $Pair\,Balanced$ & (Represents `( ) Balanced' ) \\
              &$\mid$& $L\,R$ & (Represents `( )' ) \\
    Nested &$\to$& $Unclosed\,R$ \\
    Unclosed &$\to$& $L\,Balanced$  \\
    Pair &$\to$& $L\,R$ \\
    L &$\to$& ( \\
    R &$\to$& )
  \end{tabular}
  \end{center}
\end{example}

\subsection{The CYK Algorithm}

The Cocke-Younger-Kasami (CYK) \cite{CYK1967, ParsingGuide}
algorithm is a bottom-up
parsing method that given a CFG $G$ in CNF
determines whether a string $w$ belongs
to a language $L(G)$. It operates via
dynamic programming,
constructing a triangular table
where each cell $T[i,j]$ contains the set of
non-terminals that can generate
the substring of $w$ starting at $i$ and ending at $j$.

\begin{definition}[CYK Table Construction]
  For an input string $w = w_1 w_2 \dots w_n$:
  \begin{enumerate}
      \item \textbf{Base Case:} For each $i \in \{1, \dots, n\}$,
      $T[i,i]$ contains $A$ if there is a rule $A \to w_i$.
      \item \textbf{Recursive Step ($j > i$):}
      $T[i,j]$ contains $A$ if there exists
      a rule $A \to BC$ and a split point $k$ ($i \le k < j$)
      such that $B \in T[i,k]$ and $C \in T[k+1, j]$.
  \end{enumerate}
  The string is accepted if starting symbol $S \in T[1,n]$.
\end{definition}

\begin{example}
  The \autoref{tab:cyk_triangular} illustrates a 
  CYK Table construction to 
  verify the acceptance of the string $w = \text{``()()"}$
  using the CNF grammar derived in \autoref{ex:cnf_conversion}.
  %
  The top-right cell $T[1,4]$ represents the 
  entire string
  %
  It contains the Start symbol $B$ (Balanced)
  because there is a rule `$Balanced \to Pair\,Balanced$'
  for a split point $k=2$ where $Pair\in T[1,2]$ and $Balanced\in T[3,4]$.
  
  Similarly, the cell $T[1,2]$ contains $P$ (Pair) because
  there is a rule $Pair \to L\, R$
  for a split point $k=1$ ($L\in T[1,1]$ and $R\in T[2,2]$).

  \begin{table}[h]
    \centering
    \renewcommand{\arraystretch}{1.5}
    \begin{tabular}{|c|c|c|c|}
    \hline
    $w_1=`('\,\{L\}$ & $\{B,P\}$ & $\emptyset$ & $\mathbf{\{B\}}$ \\ \cline{1-4}

    \multicolumn{1}{c|}{} & $w_2=`)'$ \{R\} & $\emptyset$ & $\emptyset$ \\ \cline{2-4}

    \multicolumn{1}{c}{} & \multicolumn{1}{c|}{} & $w_3=`('$ \{L\} & \{B,P\} \\ \cline{3-4}

    \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c|}{} & $w_4=`)'$ \{R\} \\ \cline{4-4}

    \end{tabular}
    \caption{CYK Table for $w=\text{()()}$. 
    The cells $T[i,j]$ correspond to the substring starting at $i$ and ending at $j$.
    The diagonal elements contain the input terminals and unary rules able to generate them.
    \textbf{Abbreviations:} $B$ = Balanced, $P$ = Pair}
    \label{tab:cyk_triangular}
  \end{table}
\end{example}


\subsection{Classification Problems and Formal Explanations}

Following XAI literature for classification problems
\cite{delivering_trust,darwiche2023logic}, we consider
a classifier $\kappa:~\mathbb{F}\rightarrow \mathcal{K}$
over a feature space $\mathbb{F}=\prod_{i=1}^{m} \mathbb{D}_i$
and a set of classes K.

For an instance $\mathbf{v}\in\mathbb{F}$ with prediction
$\kappa(\mathbf{v})=~c\in~\mathcal{K}$, an
\emph{abductive explanation} 
$\mathcal{X}\subseteq \mathcal{F}$ \emph{sufficient} for the prediction.
%
Formally, $\mathcal{X}$ is defined as:
%
\begin{equation}
    %
    \label{eq:axp}
    %
    \forall (\textbf{x}\in \mathbb{F}). \left[\bigwedge\nolimits_{i
    \in \mathcal{X}} (x_i=v_i)\right]\rightarrow
    \left(\kappa(\mathbf{x})=c\right)
    %
\end{equation}

Similarly, a \emph{contrastive explanation} (CXp) is a
minimal subset of features $\mathcal{Y}\subseteq\mathcal{F}$
that, if allowed to change, enables the prediction's alteration.
%
Formally, a contrastive explanation $\mathcal{Y}$ is defined as follows:
%
\begin{equation}
    %
    \label{eq:cxp}
    %
    \exists (\textbf{x}\in \mathbb{F}). \left[\bigwedge\nolimits_{j
    \not\in \mathcal{Y}} (x_j=v_j)\right] \land
    \left(\kappa(\mathbf{x}) \neq c\right)
    %
\end{equation}
%
Observe that abductive explanations are used to explain \emph{why} a
prediction is made by the classifier $\kappa$ for a given instance, while
contrastive explanations can be seen to answer \emph{why not} another
prediction is made by $\kappa$.
%
Alternatively, CXps can be seen as answering \emph{how} the
predication can be changed.

Importantly, abductive and contrastive explanations are known to enjoy
a minimal hitting set duality relationship~\cite{axpcxp}.
%
Given $\kappa(\mathbf{v})=~c$, let $\mathbb{A}_\mathbf{v}$ be the
complete set of AXps and $\mathbb{C}_\mathbf{v}$ be the complete set
of CXps for this prediction.
%
Then each AXp $\mathcal{X}\in\mathbb{A}_\mathbf{v}$ is a minimal
hitting set of $\mathbb{C}_\mathbf{v}$ and, vice versa, each CXp
$\mathcal{Y}\in\mathbb{C}_\mathbf{v}$ is a minimal hitting set of
$\mathbb{A}_\mathbf{v}$.\footnote{Given a collection of sets
$\mathbb{S}$, a \emph{hitting set} of $\mathbb{S}$ is a set $H$ such
that for each $S \in \mathbb{S}$, $H \cap S \neq \emptyset$. A
hitting set is \emph{minimal} if no proper subset of it is a
hitting set.}
%
This fact is the basis for the algorithms used for formal explanation
\emph{enumeration}~\cite{delivering_trust,ffa}.

A challenge arising in formal XAI
is given there are many AXps or CXps,
which is the \emph{best} explanation to present to a user.
An obvious answer is to use one of minimal size,
but an alternate answer is to
generate a result that records the
influence of all explanations.
A \emph{formal feature attribution} (FFA)
weighs the importance of each
feature \cite{ffa,yu-sat24} in determining the result that $\kappa(\mathbf{v}) = c$.
We define the formal feature attribution of feature $i$ as
\begin{equation}
    %
    \label{eq:ffa_definition}
    %
    \mathsf{FFA}{\mathbf{v}}(i) = \frac{ |\{ {\cal X} ~|~ {\cal X} \in \mathbb{A}_{\mathbf{v}}, i \in {\cal X} \}|
}{|\mathbb{A}_{\mathbf{v}}|}
    %
\end{equation}
The proportion of all AXps shows the
contribution of each feature to the decision.
A \emph{formal feature attribution} for $\kappa(\mathbf{v}) = c$ is the set
$\{ (i, \mathsf{FFA}{\mathbf{v}}(i)) ~|~ i \in
{\cal F}, \mathsf{FFA}{\mathbf{v}}(i)) > 0 \}$.

Critically, formal feature attribution gives an \emph{unbiased} measure of
the influence of input features on the decision.

\section{Explaining Pushdown Automata decisions}

This report describes the outgoing work
to explain the behaviour of a Pushdown Automaton
$A$ on an input $w\in\Sigma^*$, and similar to
explaining Finite Atutomata decisions
\cite{explainingFA},
this problem can be viewed as a classifier
mapping input $w$ to a class in ${\cal K} = \{ \mbox{accept},
\mbox{reject} \}$.
%
Two types ofexplanations for PDA are proposed:
abductive explanations (AXps) and contrastive
explanations (CXps).
%
Informally, an AXp answers ``why does $A$ accept/reject $w$?'';
%
a CXp answers ``How can $w$ be modified to alter the response of
$A$?''.
%


The class of explanations that are considered in this work 
are defined by regular expressions
%
\footnote{Standard regular expression notation is used.
%
$\emptyset$ denote the empty language, $c\in \Sigma$
denotes the language $\{c\}$, and $\Sigma$ the
language $\{\Sigma\}$, the set of all strings of length 1.
%
In general, given a regular expression $R$, the language it defines is
denoted by $L(R)$.
%
The concatenation of two regular expressions $R_1 R_2$ denotes the
regular language $\{ r_1 r_2 \mid r_1 \in L(R_1), r_2 \in L(R_2)\}$.
}
%
formed by replacing
some characters in $w$ by $\Sigma$

A finding in the analysis of formal explanations is the 
structural distinction between explaining \textbf{accepted} and
\textbf{rejected} words.
Typically, accepted words in Context-Free Languages
are easy to modify to make them rejected,
changing a single token often breaks
the syntactic structure, flipping the prediction to rejected.
Conversely, rejected words are often robustly wrong,
requiring multiple coordinated changes (or single specific one)
to repair the structure.

This phenomenon is illustrated using the Balanced Parentheses Grammar
$G$ and words of fixed length $n=4$.

\begin{example}[Fragility of Acceptance]
Consider the accepted word $w = \text{()()}$. In this case,
the validity relies on every token.
Modifying any single index is sufficient
to flip the prediction to \textbf{rejected}.

Every single index constitutes a CXP:
\begin{itemize}
    \item \textbf{Index 1:} Changing `(' $\to$ `)' makes $\text{))()}$ (Rejected: starts with closing).
    \item \textbf{Index 2:} Changing `)' $\to$ `(' makes $\text{((()}$ (Rejected: unmatched open).
    \item \textbf{Index 3:} Changing `(' $\to$ `)' makes $\text{()))}$ (Rejected: unmatched close).
    \item \textbf{Index 4:} Changing `)' $\to$ `(' makes $\text{()((}$ (Rejected: unmatched open).
\end{itemize}

\noindent \textbf{Conclusion for AXP:} For an accepted word,
since every feature is critical, the AXP
the subset of features required to guarantee acceptance
is the entire word.
\end{example}


The problem becomes significantly more complex for rejected words.

\begin{example}[Robustness of Rejection]
  \label{ex:rejected_parenthesis}
Consider the rejected word $w = \text{))))}$.
Here, changing a single index
is insufficient to flip the prediction to \textbf{accepted}.

There are only two Minimal CXPs for this word:
%
\begin{itemize}
    \item $\mathcal{CXP}_1 = \{1, 2\}$:
    Changing indices 1 and 2 generates the
    accepted word \underline{((}))).
    \item $\mathcal{CXP}_2 = \{1, 3\}$:
    Changing indices 1 and 3 generates the
    accepted word \underline{(})\underline{(}).
\end{itemize}
%
For this case, we can extract two meaningful AXPs
that are sufficient to \textit{guarantee} the rejection.
%
\begin{enumerate}
  %
  \item $\mathcal{AXP}_1 = \{1\}$:
  The first symbol being `)' is sufficient to guarantee rejection.
  \begin{itemize}
    %
    \item Pattern: $w' = \text{) }\Sigma\,\Sigma\,\Sigma$
    %
    \item No valid word in $G$ can begin with a closing parenthesis.
    %
    Therefore, knowing only index 1 is enough to predict ``Rejected''.
    %
  \end{itemize}
  %
  \item $\mathcal{AXP}_2 = \{2, 3\}$: The substring `))'
  at indices 2 and 3 is sufficient to guarantee
  rejection for a word of length 4.
  %
  \begin{itemize}
      %
      \item Pattern: $w' = \Sigma\text{ ) ) }\Sigma$
      %
      \item Note that there is no valid configuration
      if we have `))' in the center.
      %
      Thus, it is a sufficient condition for rejection.
      %
  \end{itemize}
\end{enumerate}

\end{example}


\begin{algorithm}[tb]
  \caption{\textsc{ExtractAXp} -- a Single AXp Extraction}
\label{alg:extract_axp}
\textbf{Input}: Candidate set $X$, PDA $\mathcal{A}$, word $w$ \\
\textbf{Output}: Minimal $X$ with

\begin{algorithmic}[1]
  \FORALL{$i \in X$} \label{alg:line:loop}
    \IF{$\axp(X\setminus\{i\})\subseteq L(\mathcal{A})$}
      \STATE $a$
    \ENDIF
  \ENDFOR
  \STATE \textbf{return} $X$
\end{algorithmic}
\end{algorithm}

Similarly to FXAI~\cite{delivering_trust,explainingFA}, we define a shorthand
notation for explanations using subsets of positions in
$w \in L(\mathcal{A})$:
%
\begin{align*}
    %
    \axp_l^u(S, w)= L(s_1 \cdots s_n) \mid s_i = w[i] \text{ if } i\in
    S \text{ else } \Sigma \\
    %
    \cxp_l^u(S, w)= L(s_1 \cdots s_n) \mid s_i = w[i] \text{ if }
    i\not\in S \text{ else } \Sigma
    %
\end{align*}

\begin{example} \label{ex:indices}
    %
    Consider the setup of Example~\ref{ex:rejected_parenthesis}.
    %
    Given the word $w=\texttt{()()}$,

    %
    \qed
    %
\end{example}
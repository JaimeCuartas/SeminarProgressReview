\chapter{Progress}

\section{Model Proposal: Context-Free Grammar (CFG) Explanations}

The research has evolved
from the study of Finite Automata (FA)
to more expressive computational models.
While FA provided a baseline for explaining
sequential behaviors,
many complex problems require the model to
``remember'' an arbitrary number of previous
inputs in the sequence
to determine the validity of subsequent inputs

consider the challenge of identifying
and explaining syntax errors in programming languages.
Code structures are inherently hierarchical;
blocks of logic are nested within one another,
defined by matching delimiters such as \{ \} in C
or ( ) in Lisp.


Consider the abstract language $L=\{a^{n}b^{n} \mid n \geq 1\}$,
which represents a sequence where every `a' must
be matched by a corresponding `b'.

\begin{itemize}
  %
  \item The Limitation: A standard
  Finite Automaton (FA) possesses no
  auxiliary memory.
  %
  Therefore, it cannot count the number
  of $a$'s to ensure they match the number
  of $b$'s once $n$ exceeds the number
  of states in the machine.
  \item The Explanation Failure:
  If an FA were used to validate such a sequence,
  it would process inputs locally.
  %
  Upon encountering a mismatch (e.g., $a^{4}b^{2}$),
  it might reject the sequence, but it lacks the
  structural context to generate a contrastive
  explanation such as:
  "The sequence is invalid because the third or fourth `a'
  was not closed by a matching `b'."
  Instead, it can only report a
  "transition failure" at the specific index,
  obscuring the root cause of the error.

\end{itemize}

\begin{itemize}
  \item \textbf{From FA to PDA:}
  We propose the use of Pushdown Automata (PDAs)
  to model decision-making processes with memory.

  Unlike FA, the addition of a stack
  allows the description of more complex
  languages.
  Here the challenge is how to explain
  PDA decisions.
\end{itemize}

\section{Motivation: The Gap between Detection and Explanation}

One well established applications of Context-Free Grammars
is in the design of programming languages and compilers.
Compilers are highly efficient at detecting when a
sequence of tokens fails to belong to a grammar.
%
However, a fundamental question is:
are they able of generating a useful \emph{explanation}
for why the failure occurred or how to fix it?

Consider the following C code, where a typo has
introduced a double opening bracket \texttt{\{\{} in the \texttt{for} loop:
%
\begin{lstlisting}[language=C]
int main(){
    for(int i=0; i<10; i++){{  // <--- Error: Double bracket
        printf("hello");
    }
}
\end{lstlisting}
%
When compiled (e.g., using GCC or Clang), the parser
consumes the input until it reaches
\textless\textless EOF\textgreater\textgreater (end-of-file),
finding only then that
EOF was not expected,
instead there is an incomplete structure, the token `\}'
is expected before \textless\textless EOF\textgreater\textgreater.
The resulting error message is:
%
\begin{verbatim}
error: expected '}' at end of input
   5 | }
     |  ^
\end{verbatim}
%
\noindent \textbf{The Explanation:}
While the compiler's output is correct,
the file ended while the stack still
contained an open brace that was not closed yet.
It is \textit{misleading}.
\begin{itemize}
  %
  \item \textbf{Root Cause:}
  The compiler points to line 5
  (the end of the file) as the location of the error.
  However, the root cause is located at line 2.
  %
  \item \textbf{Lack of Contrastive Reasoning:}
  A human (or a formal explainer) would identify
  that the input is ``almost correct''.
  %
  The explanation should not just report a
  missing symbol at the end,
  but rather propose a \textit{minimal correction}.
  %
\end{itemize}
%
In this case, the minimal correction is not to add a brace at the end,
but to remove the redundant opening brace at the loop initialization:
%
\begin{lstlisting}[language=C]
int main(){
    for(int i=0; i<10; i++){
        printf("hello");
    }
}
\end{lstlisting}
%
This discrepancy motivates the need for our proposed formal framework.
We aim to move beyond isolated decisions to
eplained decisions revealing
these minimal set of edits (Contrastive Explanations).

\section{Preliminaries} 

\textbf{Pushdown Automata and Context-Free Grammars}

To provide a foundation for the proposed explanation
extraction methods, 
standard definitions and notations
for Pushdown Automata and Context-Free Grammar are adopted
\cite{IntroAutomataTheory,HandbookTheoreticalContextFreeGrammar}.

\begin{definition}[Pushdown Automaton]
  A Pushdown Automaton (PDA) extends the capabilities of a
  Finite Automaton by incorporating an infinite memory stack.
  A PDA is formally defined as a 7-tuple
  $\mathcal{A}=(Q,\Sigma,V,\delta,q^0,v^0,F)$, where:
  \begin{itemize}
    %
    \item Q is a finite set of states.
    %
    \item $\Sigma$ is the input alphabet, a finite
    terminal alphabet.
    %
    \item $V$ is the stack alphabet, a finite
    nonterminal alphabet that can be pushed
    onto or popped from the stack.
    %
    \item $\delta:Q\times(\Sigma\cup\{\epsilon\}) \times V \rightarrow \mathcal{P}(Q \times V^*)$
    %
    \footnote{$\mathcal{P}$ denotes the power set
    (the set of all its subsets).
    %
    $V^*$ and $\Sigma^*$ denote the Kleene closures
    of the stack and input alphabets,
    respectively,
    %
    representing the sets of all finite strings formed
    by those alphabets.}
    %
    is the transition fuction.
    %
    It dictates how the machine transitions between states and
    modifies the stack based on the current state, input symbol,
    and the top symbol of the stack.
    %
    \item $q^0 \in Q$ is the initial state.
    \item $v^0 \in V$ is the initial pushdownstore symbol.
    \item $F\subseteq Q$ is the set of accepting states.
  \end{itemize}
%
  A configuration of $\mathcal{A}$ is a triple $c=(q, \gamma, x)$ in
  $Q\times V^*\times\Sigma^*$. And the automaton moves from c into configuration
  $c'=(q',\gamma', x')$, denoted as $c\vdash c'$ if:
  %
  \begin{itemize}
    %
    \item $\gamma = v \gamma_1(v\in V)$
    %
    \footnote{$\gamma_{1} \in V^*$ represents
    the remaining symbols on the
    stack below the top symbol $v$.
    %
    Thus, $\gamma$ represents the full current stack, formed by
    the top symbol $v$ (to be processed) and the rest of the stack $\gamma_1$.}
    %
    , $x=ax' (a\in \Sigma)$,
    $\gamma'= m \gamma_1(m\in V^*)$ and $(q', m)\in \delta(q, a, v)$, namely ``a-move'';
    %
    \item or $\gamma = v \gamma_1(v\in V)$, $x=x'$, $\gamma'= m \gamma_1(m\in V^*)$
    and $(q',m)\in \delta(q,\epsilon,v)$, namely ``$\epsilon$-move''.
  \end{itemize}
  %
  A \emph{word} is accepted by a pushdown automaton if, starting with an
  empty stack, there is a path through the automaton such
  that the automaton stops in an
  accepting state after the entire string has been read.
  %
  The \emph{language} recognized by a PDA $\mathcal{A}$
  is the set of all accepted words, denoted as $L(\mathcal{A})$.
  %
\end{definition}

The following PDA recognizes the
language of balanced parentheses
(a subset of the Dyck language)
%
\footnote{
  The Dyck language describes a set of strings
  with balanced and properly
  nested brackets (e.g., (), [], \{\})
  %
  \cite{HandbookTheoreticalContextFreeGrammar}.
  The example focuses solely on non-empty
  sequences of balanced ( and ).
}
%
and is used throughout the document
to illustrate the proposed ideas.
%

\begin{figure}[h]
  \centering
    \input{Figures/PDA/parenthesis_PDA.tex}
    \caption{A Pushdown Automaton $\mathcal{A}$
    accepting the language of balanced parentheses.
    The symbol \$ is used as the bottom-of-stack marker.}
  \label{fig:pda_example}
\end{figure}

\begin{example}

  Let $\mathcal{A}$ be the PDA
  that accepts the language generated by $G$ in 
  \autoref{ex:grammar_balanced}.
  The PDA uses a stack to track the depth of nesting,
  pushing a symbol for every open parenthesis and popping
  for every closed one. $B$ represents \textit{Balanced} in $G$.
  
  \begin{itemize}
      \item States: $Q = \{q_0, q_1, q_f\}$, $q^0=q_0$, $F=\{q_f\}$
      \item Input Alphabet: $\Sigma = \{(,)\}$
      \item Stack Alphabet: $V = \{ (, ), B, \$ \}$, $v^0=B$
      \item Transitions:
      \begin{enumerate}
          \item $\delta(q_0, \epsilon, \epsilon) = \{(q_1, B\$)\}$ \quad (Initialize stack with Start Symbol)
          \item $\delta(q_1, \epsilon, B) = \{(q_1, (B)B),(q_1, (B)),(q_1, ()B),(q_1, ())\}$ \quad (Expand $B$)
          \item $\delta(q_1, (, () = \{(q_1, \epsilon)\}$ \quad (Match input `(' with stack `(')
          \item $\delta(q_1, ), )) = \{(q_1, \epsilon)\}$ \quad (Match input `)' with stack `)')
          \item $\delta(q_1, \epsilon, \$) = \{(q_f, \epsilon)\}$ \quad (Accept if bottom marker is reached)
      \end{enumerate}
  \end{itemize}

  \autoref{fig:pda_example} illustrates this PDA.
  The configuration history of $\mathcal{A}$ for the input ``()()'' is:
%
  \begin{equation}
    \begin{aligned}
        & (q_0, \text{()()}, \epsilon) \\
        \vdash \ & (q_1, \text{()()}, B\$) && \text{(Initialize)} \\
        \vdash \ & (q_1, \text{()()}, ()B\$) && \text{(Expand } B \to ()B \text{)} \\
        \vdash \ & (q_1, \text{)()}, )B\$) && \text{(Match `(')} \\
        \vdash \ & (q_1, \text{()}, B\$) && \text{(Match `)')} \\
        \vdash \ & (q_1, \text{()}, ()\$) && \text{(Expand } B \to () \text{)} \\
        \vdash \ & (q_1, \text{)}, )\$) && \text{(Match `(')} \\
        \vdash \ & (q_1, \epsilon, \$) && \text{(Match `)')} \\
        \vdash \ & (q_{f}, \epsilon, \epsilon) && \text{(Accept)}
    \end{aligned}
  \end{equation}
\end{example}

\begin{definition}[Context-Free Grammar]
  A Context-Free Grammar (CFG)
  is defined as a 4-tuple $G=(V,\Sigma ,R,S)$, where:
  \begin{itemize}
    \item V (Variables/Non-terminals) is a finite set of variables (non-terminal symbols).
    \item $\Sigma$ (Terminals) is a finite set of terminal symbols, disjoint from V.
    \item R is a finite set of production rules of the
    form $A \rightarrow \alpha$, where $A\in V$ describes
    a variable and $\alpha\in(V \cup\Sigma)^*$ is a string
    of variables and terminals.
    \item $S\in V$ is the start variable.
  \end{itemize}
\end{definition}
%

A fundamental equivalence between CFGs and PDAs:
a language $L$ is context-free iff there exists a
PDA $\mathcal{A}$ such that $L(\mathcal{A})=L$
\cite{HandbookTheoreticalContextFreeGrammar, IntroAutomataTheory}.
%
This equivalence allows us to use grammar-based
parsing algorithms, such as CYK, to analyze the
behavior of PDAs.

\begin{example}
  \label{ex:grammar_balanced}
  Let $G=(\{Balanced\}, \{(,)\}, R, Balanced)$ be the grammar
  defined by the following 
  production rules $R$:
  %
  \begin{equation}
  \begin{aligned}
      Balanced &\to ( \, Balanced \, ) \, Balanced & \text{(Rule 1)}\\
      Balanced &\to ( \, Balanced \, ) & \text{(Rule 2)}\\
      Balanced &\to ( \, ) \, Balanced & \text{(Rule 3)}\\
      Balanced &\to ( \, ) & \text{(Rule 4)}
  \end{aligned}
  \end{equation}
  %
  This grammar generates the language of
  properly nested parentheses.
  For instance, the string ``()()''
  can be derivated as follows.
  %
  \begin{equation}
  \begin{aligned}
      Balanced &\Rightarrow ( \, ) \, \mathbf{Balanced}  && \text{(Rule 3: parentheses and Balanced)} \\
               &\Rightarrow ( \, ) \, ( \, ) && \text{(Rule 4: Reduce final `Balanced' to `()')}
  \end{aligned}
  \end{equation}
\end{example}




\textbf{Transition to Explainability for Pushdown Automata decisions}
%
Following the confirmation of candidacy,
a natural progression from the study of
Finite Automata is to consider computational
models with greater expressive power.
the research scope has expanded to include
Pushdown Automata (PDA).
%

\textbf{The CYK-Based Explanation Engine}

To extract formal explanations from these Context-Free
structures, we have developed a novel adaptation of the
CYK (Cocke-Younger-Kasami) algorithm.

While standard CYK is primarily used for parsing,
we utilize the resulting triangular parsing table
as a search space for explanations.

By analyzing partial trees within the CYK table,
for a a rejected string $w$ and a PDA $\mathcal{A}$
we can identify:

\begin{itemize}
  \item \emph{Minimal Abductive Explanations:}
  minimal subsets of input tokens (leaves in the parse tree)
  such that is impossible 
  for the start symbol $S$ to be derivable
  (does not appear in the top cell)
  \item \emph{Minimal Contrastive Explanations:}
  minimal set of edits (substitutions)
  required to ``repair'' the CYK table such
  that the start symbol $S$ is derivable
  (appears in the top cell).

\end{itemize}


    Minimal Contrastive Explanations: In the case of rejection, the 
Definition environment.

The Grammar $\mathcal{A}$ PCFG is defined as a tuple $G = (V, \Sigma, R, S, P)$, where:

\begin{itemize}
    \item $V$ is a finite set of non-terminal symbols.
    \item $\Sigma$ is a finite set of terminal symbols (the alphabet).
    \item $R$ is a finite set of production rules.
    \item $S \in V$ is the start symbol.
    \item $P: R \to [0, 1]$ is a probability function such that for each
$A \in V$, $\sum_{A \to \alpha \in R} P(A \to \alpha) = 1$.
\end{itemize}

The Rejected WordLet $w = \sigma_1\sigma_2...\sigma_n$
be a string in $\Sigma^*$.We say $w$ is rejected if $w \notin L(G)$, where $L(G)$
is the language generated by the grammar.


\textbf{Definition:} Contrastive Set. For a rejected word $w$ of length $n$,
a set of indices $I \subseteq \{1, \dots, n\}$ is a Contrastive Explanation if:

\begin{itemize}
    \item Feasibility: There exists a word $w' \in L(G)$
    such that $w$ and $w'$ differ only at indices $i \in I$.
    Formally, $\forall j \notin I, \sigma_j = \sigma'_j$.
    \item Minimality: No proper subset $I' \subset I$ satisfies the feasibility condition.
\end{itemize}

\textbf{Example:} For $w = ( )))$, the index set $I = \{2\}$ is a contrastive explanation
because changing index 2 to $)$ results in $w' = ( ( )) \in L(G)$, and the empty set
$\emptyset$ is not feasible.



\textbf{Introducing Probabilistic Preference}. In a PCFG, not all accepted words $w'$ are equal.
We can rank explanations by the likelihood of the correction they enable.

\textbf{The Scoring Function} 
% For any word $w' \in L(G)$, the probability $P(w')$ is the sum
% of the probabilities of all possible parse trees for
% $w'$.
For any word $w' \in L(G)$, the score $P(w')$ is the maximum probability
among all possible parse trees $T$ that yield $w'$ (Viterbi Algorithm):
$$P(w') = \max_{T \in \text{Trees}(w')} P(T)$$
\textbf{Optimal Contrastive Explanation} Given a rejected word $w$,
an explanation $I_1$ is probabilistically superior to $I_2$ if the best
correction enabled by $I_1$ is more likely than that of $I_2$.
We define the Explanation Weight as:
$$\text{Weight}(I) = \max \{ P(w') \mid w' \text{ matches } w \text{ except at indices } I, w' \in L(G) \}$$


\textbf{The Extended CYK Table}

Standard CYK populates a 3D table $T[i, j, A]$, representing the maximum probability
that non-terminal $A$ derives the substring from index $i$ to $j$.

For a word $w = \sigma_1\sigma_2...\sigma_n$ and an index set $I$:

\begin{itemize}
    \item \textbf{Base Case (Length 1)}
    For each position $i \in \{1, \dots, n\}$ and each non-terminal $A$:    
    \begin{itemize}
        \item If $i \notin I$ (Fixed):
        $$T[i, i, A] = P(A \to \sigma_i)$$
        (If no such rule exists, the probability is 0).
        \item If $i \in I$ (in cxp):
        $$T[i, i, A] = \sum_{\sigma \in \Sigma}  P(A \to \sigma) $$
        This selects the most likely terminal that $A$ can produce at that ``broken" index.
    \end{itemize}
    \item \textbf{Recursive Step (Length $l > 1$)}
    For each length $l$ from 2 to $n$, each starting position $i$ from 1 to $n-l+1$, and each non-terminal $A$:$$T[i, l, A] = \max_{A \to BC \in R} \left( \max_{1 \le k < l} \{ P(A \to BC) \cdot T[i, k, B] \cdot T[i+k, l-k, C] \} \right)$$
\end{itemize}

\textbf{Heatmap for Contrastive Explanations}


\textbf{Theorem}: Given a word $w=\sigma_1\sigma_1\dots \sigma_n$, and grammar $G$, such that $w \notin L(G)$.
Let $\mathcal{E}$ be the set of all contrastive explanations for $w$.
If $H$ is a hitting set of all contrastive explanations $\mathcal{E}$,
then no word $w'$ that keeps the indices in $H$ fixed to their original values in $w$
can be accepted by the grammar.
Mathematically:

If $\forall i \in H, \sigma'_i = \sigma_i$, then $w' \notin L(G)$.

Proof by Contradiction

\textbf{Step 1:} Assume the negation. Assume there exists a word 
$w' \in L(G)$ such that $w'$ agrees with the original rejected
word $w$ on all indices in the hitting set $H$.
$$\forall i \in H: \sigma'_i = \sigma_i$$

\textbf{Step 2:} Let $J$ be the set of indices where
$w'$ differs from $w$ ($J \cap H = \emptyset$).
$$J = \{ j \mid \sigma'_j \neq \sigma_j \}$$

\textbf{Step 3:} Relate to Contrastive Explanations.

Since $w' \in L(G)$ and it was formed by changing indices $J$ in $w$,
then by definition, $J$ is a "feasible" contrastive set.
It must either be a minimal contrastive
explanation or a superset of one.
$$\exists I \in \mathcal{E} \text{ such that } I \subseteq J$$

\textbf{Step 4:} The Contradiction.

We know from Step 2 that $J \cap H = \emptyset$.
Since $I \subseteq J$, it follows that $I \cap H = \emptyset$.
However, by definition, $H$ is a hitting set of $\mathcal{E}$,
meaning it must have a non-empty intersection with every
$I \in \mathcal{E}$.
$$H \cap I \neq \emptyset \quad (\text{Contradiction})$$

\textbf{Conclusion:} Our assumption that an accepted word $w'$ exists is false.
Therefore, the indices in $H$ effectively ``block" all possible paths to acceptance.
They are the necessary components of the error.

\begin{align*}
0.8 & \quad VP \to V \ NP   & 1   & \quad NP \to Det \ N & 0.1 & \quad N \to N \ PP   & 1 & \quad Det \to \text{the} & 0.6 & \quad N \to \text{man} \\
0.2 & \quad VP \to VP \ PP & 1   & \quad PP \to P \ NP  & 1   & \quad V \to \text{sees} & 1 & \quad P \to \text{with}   & 0.3 & \quad N \to \text{telescope}
\end{align*}

% \begin{forest}
% [VP
%   [VP
%     [V [sees]]
%     [NP
%       [Det [the]]
%       [N [man]]
%     ]
%   ]
%   [PP
%     [P [with]]
%     [NP
%       [Det [the]]
%       [N [telescope]]
%     ]
%   ]
% ]
% \end{forest}

% \begin{forest}
% [VP
%   [V [sees]]
%   [NP
%     [Det [the]]
%     [N
%       [N [man]]
%       [PP
%         [P [with]]
%         [NP
%           [Det [the]]
%           [N [telescope]]
%         ]
%       ]
%     ]
%   ]
% ]
% \end{forest}

\begin{forest}
  for tree={
    s sep=45pt,
    l sep=25pt,
  }
[VP, label=right:{\textbf{0.2}}
  [VP, label=left:{\textbf{0.8}}
    [V, label=left:{\textbf{1.0}} [sees]]
    [NP, label=right:{\textbf{1.0}}
      [Det, label=left:{\textbf{1.0}} [the]]
      [N, label=right:{\textbf{0.6}} [man]]
    ]
  ]
  [PP, label=right:{\textbf{1.0}}
    [P, label=left:{\textbf{1.0}} [with]]
    [NP, label=right:{\textbf{1.0}}
      [Det, label=left:{\textbf{1.0}} [the]]
      [N, label=right:{\textbf{0.3}} [telescope]]
    ]
  ]
]
\end{forest}

P(t1) = 0.6 * 0.8 * 0.2 * 0.3 = 0.0288

\begin{forest}
  for tree={
    s sep=45pt,
    l sep=25pt,
  }
[VP, label=left:{\textbf{0.8}}
  [V, label=left:{\textbf{1.0}} [sees]]
  [NP, label=right:{\textbf{1.0}}
    [Det, label=left:{\textbf{1.0}} [the]]
    [N, label=right:{\textbf{0.1}}
      [N, label=left:{\textbf{0.6}} [man]]
      [PP, label=right:{\textbf{1.0}}
        [P, label=left:{\textbf{1.0}} [with]]
        [NP, label=right:{\textbf{1.0}}
          [Det, label=left:{\textbf{1.0}} [the]]
          [N, label=right:{\textbf{0.3}} [telescope]]
        ]
      ]
    ]
  ]
]
\end{forest}

P(t2) = 0.6 * 0.8 * 0.1 * 0.3 = 0.0144

p(VP,sees the man with the telescope) = 0.0288 + 0.0144 = 0.0432

If ``telescope'' were ``man''

P(t3) = 0.6 * 0.8 * 0.2 * 0.6 = 0.0576

P(t4) = 0.6 * 0.8 * 0.1 * 0.6 = 0.0288

sees the man with the ? = P(t1) + P(t2) + P(t3) + P(t4) = 0.1296



Given the grammar $G$ and the rejected word 
$w=\sigma_1\sigma_2\dots \sigma_n\not\in L(G)$.
Let $\mathcal{E} = \{I_1, I_2, \dots, I_k\}$ the collection of all 
Minimal Contrastive Explanations 

Each $I \in \mathcal{E}$: ``The error happened exactly in the set of index $I$''.
and $P(I)$ Is the sum up of probabilities of all possible trees that 
derivates accepted words $w'=\sigma'_1\sigma'_2\dots \sigma'_n$
where $\forall_{i\not\in I} \sigma'_i=\sigma_i$

$P(A)$: Probability to get an accepted Tree with a minimal cxp.
Every tree is disjoint.
$P(A) = P(I_1) + P(I_2) + \dots + P(I_k)$

$$P(I \mid A) = \frac{P(I)}{\sum_{J \in \mathcal{E}} P(J)}$$

$$P(\text{Error in } i \mid A) = \sum_{I \in \mathcal{E}} P(\text{Error in } i \mid I, A) P(I \mid A)$$

$$P(\text{Error in } i \mid I, A) = \mathbb{F}(i \in I) = \begin{cases} 1 & \text{if } i \in I \\ 0 & \text{if } i \notin I \end{cases}$$

\section{Future Work and Timeline to Completion}


% \begin{figure}[tbp]
%     \begin{center}
    
%     \begin{ganttchart}[expand chart=\textwidth,
%     hgrid,
%     vgrid,
%     x unit=0.8cm,
%     y unit chart=0.7cm,
%     time slot format=simple,
%     title label font=\bfseries\small,
%     group label font=\bfseries\small,
%     bar label font=\small,
%     milestone label font=\small\itshape]{1}{12}
%     %labels
%     \gantttitle{2026 (Year 2)}{4} 
%     \gantttitle{2027 (Year 3)}{4} 
%     \gantttitle{2028 (Final)}{4} \\
%     \gantttitlelist{1,...,12}{1} \\
%     % Phase 1: Theoretical Work
%     \ganttgroup{Theoretical Refinement}{1}{3} \\
%     \ganttbar{Formal Logic for Dynamic Env}{1}{2} \\
%     \ganttmilestone{Framework Finalization}{3} \\
%     % Phase 2: System Development
%     \ganttgroup{System Implementation}{4}{7} \\
%     \ganttbar{Explainer Algorithm Dev}{4}{6} \\
%     \ganttbar{Simulator Integration}{6}{7} \\
%     % Phase 3: Validation
%     \ganttgroup{Validation \& Testing}{8}{10} \\
%     \ganttbar{Case Study: Autonomous Agents}{8}{9} \\
%     \ganttbar{Comparative Analysis}{9}{10} \\
%     % Phase 4: Thesis
%     \ganttgroup{Thesis Completion}{10}{12} \\
%     \ganttbar{Final Drafting}{10}{11} \\
%     \ganttmilestone{Thesis Submission}{12}
    

%     \end{ganttchart}
%     \end{center}
%     \caption{Gantt Chart}

% \end{figure}
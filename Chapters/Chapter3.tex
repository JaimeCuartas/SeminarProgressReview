\chapter{Maturing Theoretical Constructs and Frameworks}

\section{Model Proposal: Context-Free Grammar (CFG) Explanations}

The research has evolved
from the study of Finite Automata (FA)
to more expressive computational models.
While FA provided a baseline for explaining
sequential behaviors, they are insufficient
for dynamic environments requiring
memory or stochastic reasoning.

\begin{itemize}
  \item \textbf{From FA to PDA:}
  We propose the use of Pushdown Automata (PDAs)
  to model decision-making processes with memory.

  Unlike FA, the addition of a stack
  allows the description of more complex
  languages.
  challenging
  explanations
   to explain recursive behaviors and
  long-range dependencies in agent traces

\end{itemize}




Formal Framework for Explanations

We have matured our theoretical framework by
defining and identifying two distinct types of
formal explanations within the PDA context:

\emph{Abductive Explanations}: These identify a
minimal sufficient set of features
(or stack operations) that guarantee the
observed outcome (rejection).
%
It answers: "What specific parts of this input
were enough to cause the failure?"

\emph{Contrastive Explanations}: These identify
the minimal necessary changes to the input or
trace that would result in a different outcome
(acceptance).
%
It answers: "What is the smallest change that
would have fixed the failure?"


Implementation: Developed a Python/Cython-based
engine to automate the generation of abductive
explanations, proving that while the transition
to PDA is more challenging, it remains
computationally feasible.

\chapter{Progress Since Confirmation}

\textbf{Transition to Context-Free Explainability}
%
Following the confirmation of candidacy,
the research focus shifted from Regular Languages
(Finite Automata) to Context-Free Languages (CFLs)
and Pushdown Automata (PDA).
%
This transition was necessitated by the need to model
systems with nested dependencies and recursive logic,
which are strictly beyond the expressive power of
Finite Automata.


Definition environment.

The Grammar $\mathcal{A}$ PCFG is defined as a tuple $G = (V, \Sigma, R, S, P)$, where:

\begin{itemize}
    \item $V$ is a finite set of non-terminal symbols.
    \item $\Sigma$ is a finite set of terminal symbols (the alphabet).
    \item $R$ is a finite set of production rules.
    \item $S \in V$ is the start symbol.
    \item $P: R \to [0, 1]$ is a probability function such that for each
$A \in V$, $\sum_{A \to \alpha \in R} P(A \to \alpha) = 1$.
\end{itemize}

The Rejected WordLet $w = \sigma_1\sigma_2...\sigma_n$
be a string in $\Sigma^*$.We say $w$ is rejected if $w \notin L(G)$, where $L(G)$
is the language generated by the grammar.


\textbf{Definition:} Contrastive Set. For a rejected word $w$ of length $n$,
a set of indices $I \subseteq \{1, \dots, n\}$ is a Contrastive Explanation if:

\begin{itemize}
    \item Feasibility: There exists a word $w' \in L(G)$
    such that $w$ and $w'$ differ only at indices $i \in I$.
    Formally, $\forall j \notin I, \sigma_j = \sigma'_j$.
    \item Minimality: No proper subset $I' \subset I$ satisfies the feasibility condition.
\end{itemize}

\textbf{Example:} For $w = ( )))$, the index set $I = \{2\}$ is a contrastive explanation
because changing index 2 to $)$ results in $w' = ( ( )) \in L(G)$, and the empty set
$\emptyset$ is not feasible.



\textbf{Introducing Probabilistic Preference}. In a PCFG, not all accepted words $w'$ are equal.
We can rank explanations by the likelihood of the correction they enable.

\textbf{The Scoring Function} 
% For any word $w' \in L(G)$, the probability $P(w')$ is the sum
% of the probabilities of all possible parse trees for
% $w'$.
For any word $w' \in L(G)$, the score $P(w')$ is the maximum probability
among all possible parse trees $T$ that yield $w'$ (Viterbi Algorithm):
$$P(w') = \max_{T \in \text{Trees}(w')} P(T)$$
\textbf{Optimal Contrastive Explanation} Given a rejected word $w$,
an explanation $I_1$ is probabilistically superior to $I_2$ if the best
correction enabled by $I_1$ is more likely than that of $I_2$.
We define the Explanation Weight as:
$$\text{Weight}(I) = \max \{ P(w') \mid w' \text{ matches } w \text{ except at indices } I, w' \in L(G) \}$$


\textbf{The Extended CYK Table}

Standard CYK populates a 3D table $T[i, j, A]$, representing the maximum probability
that non-terminal $A$ derives the substring from index $i$ to $j$.

For a word $w = \sigma_1\sigma_2...\sigma_n$ and an index set $I$:

\begin{itemize}
    \item \textbf{Base Case (Length 1)}
    For each position $i \in \{1, \dots, n\}$ and each non-terminal $A$:    
    \begin{itemize}
        \item If $i \notin I$ (Fixed):
        $$T[i, i, A] = P(A \to \sigma_i)$$
        (If no such rule exists, the probability is 0).
        \item If $i \in I$ (in cxp):
        $$T[i, i, A] = \sum_{\sigma \in \Sigma}  P(A \to \sigma) $$
        This selects the most likely terminal that $A$ can produce at that ``broken" index.
    \end{itemize}
    \item \textbf{Recursive Step (Length $l > 1$)}
    For each length $l$ from 2 to $n$, each starting position $i$ from 1 to $n-l+1$, and each non-terminal $A$:$$T[i, l, A] = \max_{A \to BC \in R} \left( \max_{1 \le k < l} \{ P(A \to BC) \cdot T[i, k, B] \cdot T[i+k, l-k, C] \} \right)$$
\end{itemize}

\textbf{Heatmap for Contrastive Explanations}


\textbf{Theorem}: Given a word $w=\sigma_1\sigma_1\dots \sigma_n$, and grammar $G$, such that $w \notin L(G)$.
Let $\mathcal{E}$ be the set of all contrastive explanations for $w$.
If $H$ is a hitting set of all contrastive explanations $\mathcal{E}$,
then no word $w'$ that keeps the indices in $H$ fixed to their original values in $w$
can be accepted by the grammar.
Mathematically:

If $\forall i \in H, \sigma'_i = \sigma_i$, then $w' \notin L(G)$.

Proof by Contradiction

\textbf{Step 1:} Assume the negation. Assume there exists a word 
$w' \in L(G)$ such that $w'$ agrees with the original rejected
word $w$ on all indices in the hitting set $H$.
$$\forall i \in H: \sigma'_i = \sigma_i$$

\textbf{Step 2:} Let $J$ be the set of indices where
$w'$ differs from $w$ ($J \cap H = \emptyset$).
$$J = \{ j \mid \sigma'_j \neq \sigma_j \}$$

\textbf{Step 3:} Relate to Contrastive Explanations.

Since $w' \in L(G)$ and it was formed by changing indices $J$ in $w$,
then by definition, $J$ is a "feasible" contrastive set.
It must either be a minimal contrastive
explanation or a superset of one.
$$\exists I \in \mathcal{E} \text{ such that } I \subseteq J$$

\textbf{Step 4:} The Contradiction.

We know from Step 2 that $J \cap H = \emptyset$.
Since $I \subseteq J$, it follows that $I \cap H = \emptyset$.
However, by definition, $H$ is a hitting set of $\mathcal{E}$,
meaning it must have a non-empty intersection with every
$I \in \mathcal{E}$.
$$H \cap I \neq \emptyset \quad (\text{Contradiction})$$

\textbf{Conclusion:} Our assumption that an accepted word $w'$ exists is false.
Therefore, the indices in $H$ effectively ``block" all possible paths to acceptance.
They are the necessary components of the error.

\begin{align*}
0.8 & \quad VP \to V \ NP   & 1   & \quad NP \to Det \ N & 0.1 & \quad N \to N \ PP   & 1 & \quad Det \to \text{the} & 0.6 & \quad N \to \text{man} \\
0.2 & \quad VP \to VP \ PP & 1   & \quad PP \to P \ NP  & 1   & \quad V \to \text{sees} & 1 & \quad P \to \text{with}   & 0.3 & \quad N \to \text{telescope}
\end{align*}

% \begin{forest}
% [VP
%   [VP
%     [V [sees]]
%     [NP
%       [Det [the]]
%       [N [man]]
%     ]
%   ]
%   [PP
%     [P [with]]
%     [NP
%       [Det [the]]
%       [N [telescope]]
%     ]
%   ]
% ]
% \end{forest}

% \begin{forest}
% [VP
%   [V [sees]]
%   [NP
%     [Det [the]]
%     [N
%       [N [man]]
%       [PP
%         [P [with]]
%         [NP
%           [Det [the]]
%           [N [telescope]]
%         ]
%       ]
%     ]
%   ]
% ]
% \end{forest}

\begin{forest}
  for tree={
    s sep=45pt,
    l sep=25pt,
  }
[VP, label=right:{\textbf{0.2}}
  [VP, label=left:{\textbf{0.8}}
    [V, label=left:{\textbf{1.0}} [sees]]
    [NP, label=right:{\textbf{1.0}}
      [Det, label=left:{\textbf{1.0}} [the]]
      [N, label=right:{\textbf{0.6}} [man]]
    ]
  ]
  [PP, label=right:{\textbf{1.0}}
    [P, label=left:{\textbf{1.0}} [with]]
    [NP, label=right:{\textbf{1.0}}
      [Det, label=left:{\textbf{1.0}} [the]]
      [N, label=right:{\textbf{0.3}} [telescope]]
    ]
  ]
]
\end{forest}

P(t1) = 0.6 * 0.8 * 0.2 * 0.3 = 0.0288

\begin{forest}
  for tree={
    s sep=45pt,
    l sep=25pt,
  }
[VP, label=left:{\textbf{0.8}}
  [V, label=left:{\textbf{1.0}} [sees]]
  [NP, label=right:{\textbf{1.0}}
    [Det, label=left:{\textbf{1.0}} [the]]
    [N, label=right:{\textbf{0.1}}
      [N, label=left:{\textbf{0.6}} [man]]
      [PP, label=right:{\textbf{1.0}}
        [P, label=left:{\textbf{1.0}} [with]]
        [NP, label=right:{\textbf{1.0}}
          [Det, label=left:{\textbf{1.0}} [the]]
          [N, label=right:{\textbf{0.3}} [telescope]]
        ]
      ]
    ]
  ]
]
\end{forest}

P(t2) = 0.6 * 0.8 * 0.1 * 0.3 = 0.0144

p(VP,sees the man with the telescope) = 0.0288 + 0.0144 = 0.0432

If ``telescope'' were ``man''

P(t3) = 0.6 * 0.8 * 0.2 * 0.6 = 0.0576

P(t4) = 0.6 * 0.8 * 0.1 * 0.6 = 0.0288

sees the man with the ? = P(t1) + P(t2) + P(t3) + P(t4) = 0.1296



Given the grammar $G$ and the rejected word 
$w=\sigma_1\sigma_2\dots \sigma_n\not\in L(G)$.
Let $\mathcal{E} = \{I_1, I_2, \dots, I_k\}$ the collection of all 
Minimal Contrastive Explanations 

Each $I \in \mathcal{E}$: ``The error happened exactly in the set of index $I$''.
and $P(I)$ Is the sum up of probabilities of all possible trees that 
derivates accepted words $w'=\sigma'_1\sigma'_2\dots \sigma'_n$
where $\forall_{i\not\in I} \sigma'_i=\sigma_i$

$P(A)$: Probability to get an accepted Tree with a minimal cxp.
Every tree is disjoint.
$P(A) = P(I_1) + P(I_2) + \dots + P(I_k)$

$$P(I \mid A) = \frac{P(I)}{\sum_{J \in \mathcal{E}} P(J)}$$

$$P(\text{Error in } i \mid A) = \sum_{I \in \mathcal{E}} P(\text{Error in } i \mid I, A) P(I \mid A)$$

$$P(\text{Error in } i \mid I, A) = \mathbb{F}(i \in I) = \begin{cases} 1 & \text{if } i \in I \\ 0 & \text{if } i \notin I \end{cases}$$

\section{Future Work and Timeline to Completion}


% \begin{figure}[tbp]
%     \begin{center}
    
%     \begin{ganttchart}[expand chart=\textwidth,
%     hgrid,
%     vgrid,
%     x unit=0.8cm,
%     y unit chart=0.7cm,
%     time slot format=simple,
%     title label font=\bfseries\small,
%     group label font=\bfseries\small,
%     bar label font=\small,
%     milestone label font=\small\itshape]{1}{12}
%     %labels
%     \gantttitle{2026 (Year 2)}{4} 
%     \gantttitle{2027 (Year 3)}{4} 
%     \gantttitle{2028 (Final)}{4} \\
%     \gantttitlelist{1,...,12}{1} \\
%     % Phase 1: Theoretical Work
%     \ganttgroup{Theoretical Refinement}{1}{3} \\
%     \ganttbar{Formal Logic for Dynamic Env}{1}{2} \\
%     \ganttmilestone{Framework Finalization}{3} \\
%     % Phase 2: System Development
%     \ganttgroup{System Implementation}{4}{7} \\
%     \ganttbar{Explainer Algorithm Dev}{4}{6} \\
%     \ganttbar{Simulator Integration}{6}{7} \\
%     % Phase 3: Validation
%     \ganttgroup{Validation \& Testing}{8}{10} \\
%     \ganttbar{Case Study: Autonomous Agents}{8}{9} \\
%     \ganttbar{Comparative Analysis}{9}{10} \\
%     % Phase 4: Thesis
%     \ganttgroup{Thesis Completion}{10}{12} \\
%     \ganttbar{Final Drafting}{10}{11} \\
%     \ganttmilestone{Thesis Submission}{12}
    

%     \end{ganttchart}
%     \end{center}
%     \caption{Gantt Chart}

% \end{figure}
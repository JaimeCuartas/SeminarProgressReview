\chapter{Introduction} \label{chap:introduction}

The deplyoyment of Artificial Intelligence (AI) algorithms 
has necessitated the need for eXplainability AI (XAI) methods
in order to ensure transparency, trust, and accountability.
While much of the field has focused on heuristic explanations
for opaque models, there is an interest in formal approaches
that provide rigorous guarantees about the explanations generated
\cite{logic_based_exp, darwiche2023logic}.

A fundamental challenge in dynamic environments is
explaining sequential decision-making, To address this, 
we model these processes using Automata, which provide a
symbolic and tractable representation of sequential decision
functions. This approach allows us to generate formal explanations,
why a specific sequence of actions leads to a particular outcome. 
Automata are widely used in software verification 
\cite{PrinciplesModelChecking}, design of communication protocols
\cite{SPINModelChecker},and sintax parsing in compiler \cite{DragonBook}.
When a computational model, such as a Finite Automaton (FA) or a
Pushdown Automaton (PDA), accepts or rejects an input string,
the reasoning behind that decision can be non-trivial. 
Understanding why a specific input was accepted or rejected
is crucial for debugging, and refinement purposes.

This research project investigates the formalization 
of explanations for sequential decision-making. 
Having addressed an approach to deliver formal explanations
for Finite Automata (FA) in the first stage of this research,
and submitting it to a ICALP 2026. We now move to address
explanations for Context-Free Languages (CFG) using
Pushdown Automata (PDA).

\section{Refined scope - problem statement}

While standard XAI focuses on feature
attribution in classifiers, the "features"
in formal languages are sequential and
structural.
Since the confirmation report, the
research scope has been refined to
address three primary gaps:

\begin{itemize} 
    \item \textbf{Research Problem 1 (Completed): Explaining Finite Automata.} 
    Finite Automata are often assumed to be interpretable. However,
    large FA are cognitively inaccessible to humans.
    We have developed a framework to compute formal
    explanations for the acceptance and rejection of
    inputs in FA, providing a rigorous foundation for
    automaton-based explainability.

    \item \textbf{Research Problem 2: Explaining Pushdown Automata (PDA).} 
    Context-free languages, recognized by PDAs, introduce a stack-based memory
    that allows to represent more complex behaviors.
    %A single character's "badness" may depend on a token seen much earlier in the stream.
    My objective is the generation of 
    Minimal Contrastive Explanations (CXPs)
    the minimal sets of modifications required
    to turn a rejected word into an accepted one;
    %
    and Minimal Abductive Explanations (AXps)
    the minimal sets of tokens such that
    the word is going to be rejected.

    Finally,
    there is a lack of quantitative metrics that
    assign a ``degree of responsibility'' to
    specific indices in a rejected string.
    % The third problem focuses on the development of the Features Attribution Score (RAS),
    % using constrained optimization (Non-Negative Least Squares) to provide a probabilistic
    % ranking of which tokens most significantly contribute to a structural rejection.

    \item \textbf{Research Problem 3: Explaining Markov Decision Processes.} 
    How can the Feature Attribution Score
    be extended to explain failure states in
    Reinforcement Learning policies modeled as MDPs?
    This problem explores sequential decision-making,
    where actions influence future states and rewards.
\end{itemize}


\section{Contributions to knowledge - achieved and projected }

This research provides both theoretical and practical
contributions to the field of Computer Science:

\emph{Achieved Contributions}:
\begin{itemize}
    \item Development of an theoretical and practical approach
    to explain Finite Automata decisions.
    \item A paper submitted to ICALP 2026 tittled
    ``A Formal Framework for the Explanation of Finite Automata Decisions''
\end{itemize}

\emph{Projected Contributions}:
\begin{itemize}
    \item Explaining Pushdown Automata decisions: (In Progress)
    Extending the formal explanation framework to PDAs, which recognize context-free languages.
    This involves developing algorithms to identify the minimal contrastive explanations (CXPs)
    and Abductive Explanations (AXPs), and quantifying the contribution
    of specific tokens to the decision (acceptance and/or rejection).
    \item Explainable Reinforcement Learning via MDPs:
    Extending the formal explanation framework to Markov Decision Processes (MDPs).
    The goal is to provide verifiable explanations for failure states in Reinforcement Learning policies
    treating the policy as a stochastic process and identifying the specific environmental factors 
    or decision points that lead to a particular outcomes.
\end{itemize}



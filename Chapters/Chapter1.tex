\chapter{Introduction} \label{chap:introduction}

The deployment of Artificial Intelligence (AI) algorithms 
has necessitated the need for eXplainability AI (XAI) methods
in order to ensure transparency, trust, and accountability.
While much of the field has focused on heuristic explanations
for opaque models, there is an interest in formal approaches
that provide rigorous guarantees about the explanations generated
\cite{logic_based_exp, darwiche2023logic}.

A fundamental challenge in dynamic environments is
explaining sequential decision-making.
%
To address this, 
we model these processes using Automata, which provide a
symbolic and tractable representation of sequential decision
functions.
%
This approach allows us to generate formal explanations,
why a specific sequence of actions leads to a particular outcome. 
Automata are widely used in software verification 
\cite{PrinciplesModelChecking}, design of communication protocols
\cite{SPINModelChecker},and sintax parsing in compiler \cite{DragonBook}.
When a computational model, such as a Finite Automaton (FA) or a
Pushdown Automaton (PDA), accepts or rejects an input string,
the reasoning behind that decision can be non-trivial.
%
Understanding why a specific input was accepted or rejected
is crucial for debugging, and refinement purposes.

This research project investigates the formalization 
of explanations for sequential decision-making. 
%
Having addressed an approach to deliver formal explanations
for Finite Automata (FA) in the first stage of this research,
and submitting it to a ICALP 2026.
%
I now move to address
explanations for Context-Free Languages (CFG) /
Pushdown Automata (PDA) decisions.

Current literature focuses on the
\textit{performance} of parsing rather than the
\textit{interpretability} of the decision.
%
Modern parsing algorithms,
such as Tree-sitter \cite{tree_sitter}
and ANTLR 4 \cite{antlr},
utilise sophisticated incremental parsing and adaptive LL(*)
algorithms to ensure low-latency feedback for
integrated development environments (IDEs).
%
While these methods are efficient at error recovery,
often scaling linearly with input size, they treat the
decision process as the main objective.


\section{Refined scope - problem statement}

While standard XAI focuses on feature
attribution in classifiers, the "features"
in formal languages are sequential and
structural.
%
Since the confirmation report, the
research scope has been refined to
address three primary gaps:

\begin{itemize}
    %
    \item \textbf{Research Problem 1 (Completed): Explaining Finite Automata.} 
    How can we provide Formal Explanations 
    for Finite Automata decisions?
    %
    \begin{itemize}
        %
        \item To Define Formal explanations for
        understanding the acceptance/rejection of
        inputs in Finite Automata
        %
        \item To develop a method to compute
        Formal Explanations for Finite Automata
        %
    \end{itemize}
    %
    \item \textbf{Research Problem 2: Explaining Pushdown Automata (PDA).}
    %
    How can we provide Formal Explanations
    for Pushdown Automata decisions?
    %
    \begin{itemize}
        %
        \item To Define Formal explanations for
        understanding the acceptance/rejection of
        inputs in Pushdown Automata.
        %
        \item To develop a method to compute
        Formal Explanations for Pushdown Automata.
        %
        \item To propose a method to identify
        the most significant or ``most likely''
        explanations among explanations.
        %
    \end{itemize}
    %
    \item \textbf{Research Problem 3: Explaining Decisions of Sthocastic models.} 
    %
    \begin{itemize}
        %
        \item To Define Formal explanations for understanding
        the probable outcomes in Stochastic Models.
        %
        \item To develop a method to compute Formal Explanations
        for Stochastic Models
        (e.g., Probabilistic Finite Automata or Markov Models).
        %
        \item To propose a method to identify
        the most significant or ``most likely''
        explanations among probabilistic paths.
        %
    \end{itemize}
\end{itemize}


\section{Contributions - achieved and projected }

This research provides both theoretical and practical
contributions to the field of Computer Science:

\emph{Achieved Contributions}:
\begin{itemize}
    \item Development of a theoretical and practical approach
    to explain Finite Automata decisions.
    \item A paper submitted to ICALP 2026 tittled
    ``A Formal Framework for the Explanation of Finite Automata Decisions''
\end{itemize}

\emph{Projected Contributions}:
\begin{itemize}
    \item Explaining Pushdown Automata decisions: (In Progress)
    Extending the formal explanation framework to PDAs,
    which recognize context-free languages.
    %
    This involves developing algorithms to identify the
    minimal contrastive explanations (CXPs)
    and Abductive Explanations (AXPs),
    and quantifying the contribution
    of specific tokens to the decision (acceptance or rejection).
    %
    \item Explaining Decisions of Sthocastic models:
    Extending the formal explanation framework to Stochastic Models.
    %
    The goal is to provide verifiable explanations able to
    identify the environmental factors
    or decision points that lead to a particular outcomes.
\end{itemize}



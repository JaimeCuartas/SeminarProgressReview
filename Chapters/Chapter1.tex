\chapter{Introduction} \label{chap:introduction}

The deplyoyment of Artificial Intelligence (AI) algorithms 
has necessitated the need for eXplainability AI (XAI) methods
in order to ensure transparency, trust, and accountability.
While much of the field has focused on heuristic explanations
for opaque models, there is an interest in formal approaches
that provide rigorous guarantees about the explanations generated
\cite{logic_based_exp, darwiche2023logic}.

A fundamental challenge in dynamic environments is
explaining sequential decision-making, To address this, 
we model these processes using Automata, which provide a
symbolic and tractable representation of sequential decision
functions. This approach allows us to generate formal explanations,
why a specific sequence of actions leads to a particular outcome. 
Automata are widely used in software verification 
\cite{PrinciplesModelChecking}, design of communication protocols
\cite{SPINModelChecker},and sintax parsing in compiler \cite{DragonBook}.
When a computational model, such as a Finite Automaton (FA) or a
Pushdown Automaton (PDA), accepts or rejects an input string,
the reasoning behind that decision can be non-trivial. 
Understanding why a specific input was accepted or rejected
is crucial for debugging, and refinement purposes.

This research project investigates the formalization 
of explanations for sequential decision-making. 
Having addressed an approach to deliver formal explanations
for Finite Automata (FA) in the first stage of this research,
and submitting it to a ICALP 2026. We now move to address
explanations for Context-Free Languages (CFG) using
Pushdown Automata (PDA).

\section{Refined scope - problem statement}

While standard XAI focuses on feature
attribution in classifiers, the "features"
in formal languages are sequential and
structural.
Since the confirmation report, the
research scope has been refined to
address three primary gaps:

\begin{itemize} 
    \item \textbf{Research Problem 1 (Completed): Explaining Finite Automata.} 
    Finite Automata are often assumed to be interpretable. However,
    large FA are cognitively inaccessible to humans.
    We have developed a framework to compute formal
    explanations for the acceptance and rejection of
    inputs in FA, providing a rigorous foundation for
    automaton-based explainability.

    \item \textbf{Research Problem 2: Explaining Pushdown Automata (PDA).} 
    Context-free languages, recognized by PDAs, introduce a stack-based memory
    that allows to represent makes explanations more complex.
    A single character's "badness" may depend on a token seen much earlier in the stream.
    The second problem addresses the generation of Minimal Contrastive Explanations (CXPs)
    the minimal sets of modifications required to turn a rejected word into an accepted one.

    There is a lack of quantitative metrics that assign a "degree of responsibility" to
    specific indices in a rejected string. 
    The third problem focuses on the development of the Features Attribution Score (RAS),
    using constrained optimization (Non-Negative Least Squares) to provide a probabilistic
    ranking of which tokens most significantly contribute to a structural rejection.

    \item \textbf{Research Problem 3: Explaining Markov Decision Processes.} 
    How can the Feature Attribution Score be extended to explain failure states in
    Reinforcement Learning policies modeled as MDPs?
    This problem explores the adaptation of RAS to sequential decision-making,
    where actions influence future states and rewards.
\end{itemize}


\section{Contributions to knowledge - achieved and projected }
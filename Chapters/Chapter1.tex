\chapter{Introduction} \label{chap:introduction}

The deployment of Artificial Intelligence (AI) algorithms 
has necessitated the need for eXplainability AI (XAI) methods
to ensure transparency, trust, and accountability.
While much of the field has focused on heuristic explanations
for opaque models, there is an interest in formal approaches
that provide rigorous guarantees about the explanations generated
\cite{logic_based_exp, darwiche2023logic}.

A fundamental challenge in dynamic environments is
explaining sequential decision-making.
%
To address this, 
we model these processes using Automata, which provide a
symbolic and tractable representation of sequential decision
functions.
%
This approach allows us to generate formal explanations,
why a specific sequence of actions leads to a particular outcome. 
Automata are widely used in software verification 
\cite{PrinciplesModelChecking}, design of communication protocols
\cite{SPINModelChecker}, and syntax parsing in compiler \cite{DragonBook}.
When a computational model, such as a Finite Automaton (FA) or a
Pushdown Automaton (PDA), accepts or rejects an input string,
the reasoning behind that decision can be non-trivial.
%
Understanding why a specific input was accepted or rejected
is crucial for debugging, and refinement purposes.

This research project investigates the formalisation 
of explanations for sequential decision-making. 
%
Having developed an approach to deliver formal explanations
for Finite Automata (FA) during the first stage of this research,
submitted to ICALP 2026.
%
I now move to address
explanations for Context-Free Grammar (CFG)
decisions.

Current literature focuses on the
\textit{performance} of parsing rather than the
\textit{interpretability} of the decision.
%
Modern parsing algorithms,
such as Tree-sitter \cite{tree_sitter}
and ANTLR 4 \cite{antlr},
utilise sophisticated incremental parsing and adaptive LL(*)
algorithms to ensure low-latency feedback for
integrated development environments (IDEs).
%
While these methods are efficient at 
error recovery \cite{DragonBook, MinimumEditDistancePDA},
often scaling linearly with input size, they treat the
decision process as the main objective.
%
In this work, we prioritise interpretability by
formally defining and computing the minimal reasons
for a parsing error with minimal sets of
corrections required to resolve it.
